{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import FAISS  # FAISSë¡œ ë³€ê²½\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory\n",
    "\n",
    "# ì˜¤í”ˆAI API í‚¤ ì„¤ì •\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# cache_resourceë¡œ í•œë²ˆ ì‹¤í–‰í•œ ê²°ê³¼ ìºì‹±í•´ë‘ê¸°\n",
    "@st.cache_resource\n",
    "def load_and_split_pdf(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    return loader.load_and_split()\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì„ FAISS ì•ˆì— ì„ë² ë”© ë²¡í„°ë¡œ ì €ì¥\n",
    "@st.cache_resource\n",
    "def create_vector_store(_docs):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    split_docs = text_splitter.split_documents(_docs)\n",
    "    index_path = \"./faiss_index\"\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        split_docs, \n",
    "        OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "    )\n",
    "    # FAISSëŠ” ì €ì¥ ì‹œ save_local ë©”ì„œë“œ ì‚¬ìš©\n",
    "    vectorstore.save_local(index_path)\n",
    "    return vectorstore\n",
    "\n",
    "# ë§Œì•½ ê¸°ì¡´ì— ì €ì¥í•´ë‘” FAISSê°€ ìˆëŠ” ê²½ìš°, ì´ë¥¼ ë¡œë“œ\n",
    "@st.cache_resource\n",
    "def get_vectorstore(_docs):\n",
    "    index_path = \"./faiss_index\"\n",
    "    if os.path.exists(index_path):\n",
    "        return FAISS.load_local(\n",
    "            index_path,\n",
    "            OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "        )\n",
    "    else:\n",
    "        return create_vector_store(_docs)\n",
    "    \n",
    "# PDF ë¬¸ì„œ ë¡œë“œ-ë²¡í„° DB ì €ì¥-ê²€ìƒ‰ê¸°-íˆìŠ¤í† ë¦¬ ëª¨ë‘ í•©ì¹œ Chain êµ¬ì¶•\n",
    "@st.cache_resource\n",
    "def initialize_components(selected_model):\n",
    "    file_path = r\"../data/ëŒ€í•œë¯¼êµ­í—Œë²•(í—Œë²•)(ì œ00010í˜¸)(19880225).pdf\"\n",
    "    pages = load_and_split_pdf(file_path)\n",
    "    vectorstore = get_vectorstore(pages)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    # ì±„íŒ… íˆìŠ¤í† ë¦¬ ìš”ì•½ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸\n",
    "    contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "    which might reference context in the chat history, formulate a standalone question \\\n",
    "    which can be understood without the chat history. Do NOT answer the question, \\\n",
    "    just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", contextualize_q_system_prompt),\n",
    "            MessagesPlaceholder(\"history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # ì§ˆë¬¸-ë‹µë³€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸\n",
    "    qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "    Use the following pieces of retrieved context to answer the question. \\\n",
    "    If you don't know the answer, just say that you don't know. \\\n",
    "    Keep the answer perfect. please use imogi with the answer.\n",
    "    ëŒ€ë‹µì€ í•œêµ­ì–´ë¡œ í•˜ê³ , ì¡´ëŒ“ë§ì„ ì¨ì¤˜.\\\n",
    "\n",
    "    {context}\"\"\"\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", qa_system_prompt),\n",
    "            MessagesPlaceholder(\"history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    llm = ChatOpenAI(model=selected_model)\n",
    "    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "    return rag_chain\n",
    "\n",
    "# Streamlit UI\n",
    "st.header(\"í—Œë²• Q&A ì±—ë´‡ ğŸ’¬ ğŸ“š\")\n",
    "option = st.selectbox(\"Select GPT Model\", (\"gpt-4o-mini\", \"gpt-3.5-turbo-0125\"))\n",
    "rag_chain = initialize_components(option)\n",
    "chat_history = StreamlitChatMessageHistory(key=\"chat_messages\")\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    lambda session_id: chat_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")\n",
    "\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state[\"messages\"] = [{\"role\": \"assistant\", \n",
    "                                     \"content\": \"í—Œë²•ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!\"}]\n",
    "\n",
    "for msg in chat_history.messages:\n",
    "    st.chat_message(msg.type).write(msg.content)\n",
    "\n",
    "\n",
    "if prompt_message := st.chat_input(\"Your question\"):\n",
    "    st.chat_message(\"human\").write(prompt_message)\n",
    "    with st.chat_message(\"ai\"):\n",
    "        with st.spinner(\"Thinking...\"):\n",
    "            config = {\"configurable\": {\"session_id\": \"any\"}}\n",
    "            response = conversational_rag_chain.invoke(\n",
    "                {\"input\": prompt_message},\n",
    "                config)\n",
    "            \n",
    "            answer = response['answer']\n",
    "            st.write(answer)\n",
    "            with st.expander(\"ì°¸ê³  ë¬¸ì„œ í™•ì¸\"):\n",
    "                for doc in response['context']:\n",
    "                    st.markdown(doc.metadata['source'], help=doc.page_content)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
